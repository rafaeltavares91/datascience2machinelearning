{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pGCUzMmJING"
   },
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JgRfEve-Jj3O"
   },
   "source": [
    "\n",
    "Over the past lesson, we learned about **Decision Trees**, and looked at ways to reduce overfitting. The most powerful tool for reducing **Decision Tree** overfitting is called the **Random Forest** algorithm. In this lesson, we'll learn how to construct and apply **Random Forests**.\n",
    "\n",
    "We'll continue using the same 1994 census data set on U.S. income. It contains information on **marital status**, **age**, **type of work**, and **more**. The target column, **high_income**, records salaries less than or equal to 50k a year (0), and more than 50k a year (1).\n",
    "\n",
    "You can download the data from the [University of California](http://archive.ics.uci.edu/ml/datasets/Adult), Irvine's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1542612569372,
     "user": {
      "displayName": "Ivanovitch Silva",
      "photoUrl": "https://lh4.googleusercontent.com/-baHwkIBEacY/AAAAAAAAAAI/AAAAAAAAFo0/aWRaXNQgy7Q/s64/photo.jpg",
      "userId": "06428777505436195303"
     },
     "user_tz": 180
    },
    "id": "1nwfztMMK-gH",
    "outputId": "072f52c7-c67e-4964-91bd-b387cb2f540a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>high_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education_num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital_status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country high_income  \n",
       "0          2174             0              40   United-States       <=50K  \n",
       "1             0             0              13   United-States       <=50K  \n",
       "2             0             0              40   United-States       <=50K  \n",
       "3             0             0              40   United-States       <=50K  \n",
       "4             0             0              40            Cuba       <=50K  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reading the dataset\n",
    "income = pd.read_csv(\"income.csv\")\n",
    "income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 636,
     "status": "ok",
     "timestamp": 1542612575144,
     "user": {
      "displayName": "Ivanovitch Silva",
      "photoUrl": "https://lh4.googleusercontent.com/-baHwkIBEacY/AAAAAAAAAAI/AAAAAAAAFo0/aWRaXNQgy7Q/s64/photo.jpg",
      "userId": "06428777505436195303"
     },
     "user_tz": 180
    },
    "id": "SI2P3tfRLkht",
    "outputId": "9d46ec11-53e2-41bd-a85f-63f857c7c09f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education_num     0\n",
       "marital_status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital_gain      0\n",
       "capital_loss      0\n",
       "hours_per_week    0\n",
       "native_country    0\n",
       "high_income       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying if there are missing values\n",
    "income.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1542612579303,
     "user": {
      "displayName": "Ivanovitch Silva",
      "photoUrl": "https://lh4.googleusercontent.com/-baHwkIBEacY/AAAAAAAAAAI/AAAAAAAAFo0/aWRaXNQgy7Q/s64/photo.jpg",
      "userId": "06428777505436195303"
     },
     "user_tz": 180
    },
    "id": "4MMoUfW6K9_x",
    "outputId": "84dd10ed-452d-4a22-b9ad-d75bd6b6bfdd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>high_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>77516</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>83311</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>215646</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>234721</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>338409</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt  education  education_num  marital_status  \\\n",
       "0   39          7   77516          9             13               4   \n",
       "1   50          6   83311          9             13               2   \n",
       "2   38          4  215646         11              9               0   \n",
       "3   53          4  234721          1              7               2   \n",
       "4   28          4  338409          9             13               2   \n",
       "\n",
       "   occupation  relationship  race  sex  capital_gain  capital_loss  \\\n",
       "0           1             1     4    1          2174             0   \n",
       "1           4             0     4    1             0             0   \n",
       "2           6             1     4    1             0             0   \n",
       "3           6             0     2    1             0             0   \n",
       "4          10             5     2    0             0             0   \n",
       "\n",
       "   hours_per_week  native_country  high_income  \n",
       "0              40              39            0  \n",
       "1              13              39            0  \n",
       "2              40              39            0  \n",
       "3              40              39            0  \n",
       "4              40               5            0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform nominal columns to Categorical\n",
    "for name in [\"workclass\",\"education\", \"marital_status\", \"occupation\", \"relationship\", \n",
    "             \"race\", \"sex\", \"native_country\", \"high_income\"]:\n",
    "    col = pd.Categorical(income[name])\n",
    "    income[name] = col.codes\n",
    "income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J7zcaPQsK9qQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Set a random seed so the shuffle is the same every time\n",
    "np.random.seed(1)\n",
    "\n",
    "# Shuffle the rows  \n",
    "# This permutes the index randomly using numpy.random.permutation\n",
    "# Then, it reindexes the dataframe with the result\n",
    "# The net effect is to put the rows into random order\n",
    "income = income.reindex(np.random.permutation(income.index))\n",
    "\n",
    "train_max_row = math.floor(income.shape[0] * .8)\n",
    "train = income.iloc[:train_max_row]\n",
    "test = income.iloc[train_max_row:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vYxZAUTkJINY"
   },
   "source": [
    "## Combining Model Predictions With Ensembles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fAGUwsD8M2no"
   },
   "source": [
    "A **Random Forest** is a kind of ensemble model. [Ensembles](https://en.wikipedia.org/wiki/Ensemble_learning) combine the predictions of multiple models to create a more accurate final prediction. We'll make a simple ensemble to see how they work.\n",
    "\n",
    "Let's create two decision trees with slightly different parameters:\n",
    "\n",
    "- One with **min_samples_leaf** set to 2\n",
    "- One with **max_depth** set to 5\n",
    "\n",
    "Then, we'll check their accuracies separately. On the next screen, we'll combine their predictions and compare the combined accuracy with the individual accuracies of both trees.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Fit both **clf** and **clf2** to the data.\n",
    "  - Use **train[columns]** as the predictors, and **train[\"high_income\"]** as the target.\n",
    "- Make predictions on the test set predictors (**test[columns]**) using both **clf** and **clf2**.\n",
    "- For both sets of predictions, compute the AUC between the predictions and the actual values (**test[\"high_income\"]**) using the [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) function.\n",
    "- Use the **print()** function to display the AUC values for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hw58j8aeOkLh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "           \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "clf2 = DecisionTreeClassifier(random_state=1, max_depth=5)\n",
    "clf2.fit(train[columns], train[\"high_income\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6878964226062301\n",
      "0.6759853906508785\n"
     ]
    }
   ],
   "source": [
    "predict1 = clf.predict(test[columns])\n",
    "\n",
    "predict2 = clf2.predict(test[columns])\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(roc_auc_score(test['high_income'], predict1))\n",
    "print(roc_auc_score(test['high_income'], predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1Ov7SwhJINd"
   },
   "source": [
    "## Combining Our Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8jyxCq1PbvE"
   },
   "source": [
    "\n",
    "\n",
    "When we have multiple classifiers making predictions, we can treat each set of predictions as a column in a matrix. Here's an example where we have Decision Tree 1 (DT1), Decision Tree 2 (DT2), and Decision Tree 3 (DT3):\n",
    "\n",
    "```python\n",
    "DT1     DT2    DT3\n",
    "0       1      0\n",
    "1       1      1\n",
    "0       0      1\n",
    "1       0      0\n",
    "```\n",
    "\n",
    "Whenever we add more models to our ensemble, we just add more columns to the combined predictions. Ultimately, we don't want this matrix, though -- we want one prediction per row in the training data. To accomplish this, we'll need to create rules to convert each row of our matrix of predictions into a single number.\n",
    "\n",
    "We want to create a **Final Prediction** vector that looks like this:\n",
    "\n",
    "```python\n",
    "DT1     DT2    DT3    Final Prediction\n",
    "0       1      0      0\n",
    "1       1      1      1\n",
    "0       0      1      0\n",
    "1       0      0      0\n",
    "```\n",
    "\n",
    "There are many ways to get from the output of multiple models to a final vector of predictions. One method is [**majority voting**](http://www.scholarpedia.org/article/Ensemble_learning#Voting_based_methods), in which each classifier gets a \"vote,\" and the most commonly voted value for each row \"wins.\" This only works if there are more than two classifiers (and ideally an odd number, so we don't have to write a rule to break ties). Majority voting is what we applied in the example above.\n",
    "\n",
    "**Because we only had two classifiers on the last section**, we'll have to use a different method to combine predictions. We'll take the **mean** of all of the items in a row. Right now, we're using the **predict()** method, which returns either 0 or 1. **predict()** returns something like this:\n",
    "\n",
    "```python\n",
    "0\n",
    "1\n",
    "0\n",
    "1\n",
    "```\n",
    "\n",
    "We can use the **RandomForestClassifier.predict_proba()** method instead, which will predict a probability from 0 to 1 that a given class is the right one for a row. Because 0 and 1 are our two classes, we'll get a matrix containing the number of rows in the income dataframe, and two columns. **predict_proba()** will return a result that looks like this:\n",
    "\n",
    "\n",
    "```python\n",
    "0     1\n",
    ".7    .3\n",
    ".2    .8\n",
    ".1    .9\n",
    "```\n",
    "\n",
    "Each row will correspond to a prediction. The first column is the probability that the prediction is a 0, and the second column is the probability that the prediction is a 1. Each row adds up to 1.\n",
    "\n",
    "If we just take the second column, we get the average value that the classifier would predict for that row. If there's a .9 probability that the correct classification is 1, we can use the .9 as the value the classifier is predicting. This will give us a continuous output in a single vector, instead of just 0 or 1.\n",
    "\n",
    "Then we can add together all of the vectors we get through this method, and divide the sum by the total number of vectors to get the mean prediction made across the entire ensemble for a particular row. Finally, we round off to get a 0 or 1 prediction for the row.\n",
    "\n",
    "If we use the **predict_proba()** method on both classifiers from the last screen to generate probabilities, take the mean for each row, and then round the results, we'll get ensemble predictions.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Add **predictions** and **predictions2**, then divide by 2 to get the **mean**.\n",
    "- Use **numpy.round()** to round all of the resulting predictions. Store the result in **rounded**.\n",
    "- Print the resulting **AUC** score between the actual values(**test[\"high_income\"]**) and the predictions (**rounded**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "csUHMwO_5Z_D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7150846804038882\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict_proba(test[columns])[:,1]\n",
    "predictions2 = clf2.predict_proba(test[columns])[:,1]\n",
    "\n",
    "mean = (predictions + predictions2) / 2\n",
    "print(roc_auc_score(test['high_income'], np.round(mean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfQ7eFJ6JINk"
   },
   "source": [
    "## Why Ensembling Works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5JU14UL9KJ9"
   },
   "source": [
    "\n",
    "\n",
    "As we can see from the previous section, the combined predictions of the two trees had a higher AUC than that of either tree on its own:\n",
    "\n",
    "| settings             | test AUC |\n",
    "|----------------------|----------|\n",
    "| min_samples_leaf: 2  | 0.688    |\n",
    "| max_depth: 2         | 0.676    |\n",
    "| combined predictions | 0.715    |\n",
    "\n",
    "To intuitively understand why this makes sense, think about two people with the same level of talent. One learned programming in college, while the other learned it on her own.\n",
    "\n",
    "If we give both of them the same project, they'll approach it in slightly different ways, due to their different experiences. They may both produce code that achieves the same result, but one may run faster in certain areas. The other may have a better interface. Even though both of them have about the same level of talent, their solutions are stronger in different areas because they approached the problem differently.\n",
    "\n",
    "If we combine the best parts of both of their projects, we'll end up with a stronger combined project.\n",
    "\n",
    "Ensembling is exactly the same. The models are approaching the same problem in slightly different ways, and building different trees because we used different parameters for each one. Each tree makes different predictions in different areas. Even though both trees have about the same accuracy, when we combine them, the result is stronger because it leverages the strengths of both approaches.\n",
    "\n",
    "The more \"diverse\" or dissimilar the models we use to construct an ensemble are, the stronger their combined predictions will be (assuming that all of the models have about the same accuracy). **Ensembling a decision tree and a logistic regression model, for example, will result in stronger predictions than ensembling two decision trees with similar parameters.** That's because those two models use very different approaches to arrive at their answers.\n",
    "\n",
    "On the other hand, if the models we ensemble are very similar in how they make predictions, ensembling will result in a negligible boost.\n",
    "\n",
    "**Ensembling models with very different accuracies generally won't improve overall accuracy.** Ensembling a model with a .75 AUC and a model with a .85 AUC on a test set will usually result in an AUC somewhere in between the two original values. There's a way around this that we'll discuss later on, which we call **weighting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JgMFJs_DJINn"
   },
   "source": [
    "## Introducing Variation With Bagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTxQEhSM-Iqg"
   },
   "source": [
    "\n",
    "\n",
    "A **Random Forest** is an ensemble of **Decision Trees**. If we don't make any modifications to the trees, each tree will be exactly the same, so we'll get no boost when we ensemble them. In order to make ensembling effective, **we have to introduce variation** into each individual decision tree model.\n",
    "\n",
    "If we introduce variation, each tree will be be constructed slightly differently, and will therefore make different predictions. **This variation is what puts the \"random\" in \"Random Forest.\"**\n",
    "\n",
    "There are **two main ways to introduce variation** in a Random Forest -- **bagging** and **random feature subsets**. We'll dive into bagging first.\n",
    "\n",
    "In a Random Forest, we don't train each tree on the entire data set. We train it on a random sample of the data, or a \"bag,\" instead. We perform this sampling with replacement, which means that after we select a row from the data we're sampling, we put the row back in the data so it can be picked again. Some rows from the original data may appear in the \"bag\" multiple times.\n",
    "\n",
    "Let's use bagging with the first tree we trained.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- **predictions** is a list of vectors corresponding to predictions on the test set.\n",
    "- Use the technique we employed earlier to add all of the vectors together, and divide by 10 to get the **mean** prediction for each row.\n",
    "- Use **numpy.round()** to round the resulting predictions.\n",
    "- Finally, print the **AUC** score between the combined predictions and **test[\"high_income\"].**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJLWYeyG_-Dy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7329963297474371\n"
     ]
    }
   ],
   "source": [
    "# We'll build 10 trees\n",
    "tree_count = 10\n",
    "\n",
    "# Each \"bag\" will have 60% of the number of original rows\n",
    "bag_proportion = .6\n",
    "\n",
    "predictions = []\n",
    "for i in range(tree_count):\n",
    "    # We select 60% of the rows from train, sampling with replacement\n",
    "    # We set a random state to ensure we'll be able to replicate our results\n",
    "    # We set it to i instead of a fixed value so we don't get the same sample\n",
    "    # in every loop. That would make all of our trees the same\n",
    "    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)\n",
    "    \n",
    "    # Fit a decision tree model to the \"bag\"\n",
    "    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)\n",
    "    clf.fit(bag[columns], bag[\"high_income\"])\n",
    "    \n",
    "    # Using the model, make predictions on the test data\n",
    "    predictions.append(clf.predict_proba(test[columns])[:,1])\n",
    "    \n",
    "mean = sum(predictions) / len(predictions)\n",
    "print(roc_auc_score(test['high_income'], np.round(mean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BAqVtopsJIN0"
   },
   "source": [
    "## Selecting Random Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHEI8IE7B0-Z"
   },
   "source": [
    "\n",
    "\n",
    "Using the **bagging** example from the previous section, we gained some accuracy over a single **Decision Tree**. To be exact, we achieved an AUC score of around .733 with **bagging**, which is an improvement over the AUC score of .688 we got **without bagging**:\n",
    "\n",
    "| settings                          | test AUC |\n",
    "|-----------------------------------|----------|\n",
    "| min_samples_leaf: 2               | 0.688    |\n",
    "| max_depth: 2                      | 0.676    |\n",
    "| combined predictions              | 0.715    |\n",
    "| min_samples_leaf: 2, with bagging | 0.732    |\n",
    "\n",
    "Let's go back to the **Decision Tree** algorithm we explored in last lesson to explain random feature subsets:\n",
    "\n",
    "- First we pick the maximum number of features we want to evaluate each time we split the tree.\n",
    "    - This has to be less than the total number of columns in the data.\n",
    "- Every time we split, we pick a random sample of features from the data.\n",
    "- Then we compute the information gain for each feature in our random sample, and pick the one with the highest information gain to split on.\n",
    "To understand how splits work, let's look at information gain or entropy. Entropy is the measure of \"disorder\" in the data set. If a dataset has all the same labels, they'll have low entropy. If all the labels are different, they'll have high entropy. Splits that give us more information about the data, will ideally minimize entropy. In other words, the tree will ideally split the labels into distinct groups with as little mixture possible. This'll allow the splits to give our tree more predictive power.\n",
    "\n",
    "We're repeating the same process to select the optimal split that minimizes entropy for a node. However, we'll only evaluate a constrained set of features that we select randomly. This introduces variation into the trees, and makes for more powerful ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5S2bxpGJIN6"
   },
   "source": [
    "## Random Subsets in scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWW_X7FHD3qt"
   },
   "source": [
    "\n",
    "\n",
    "We can also repeat our random subset selection process in scikit-learn. We just set the **splitter** parameter on **DecisionTreeClassifier** to **\"random\"**, and the **max_features** parameter to **\"auto\"**. If we have N columns, this will pick a subset of features of size $\\sqrt{N}$, compute the Gini coefficient for each (this is similar to information gain), and split the node on the best column in the subset.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Modify the instantiation of the **DecisionTreeClassifier** object.\n",
    "- Set **splitter** to **\"random\"**, and **max_features** to **\"auto\"**.\n",
    "- Print the resulting **AUC** score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1232,
     "status": "ok",
     "timestamp": 1542616027118,
     "user": {
      "displayName": "Ivanovitch Silva",
      "photoUrl": "https://lh4.googleusercontent.com/-baHwkIBEacY/AAAAAAAAAAI/AAAAAAAAFo0/aWRaXNQgy7Q/s64/photo.jpg",
      "userId": "06428777505436195303"
     },
     "user_tz": 180
    },
    "id": "b8W9iHjbE4rx",
    "outputId": "fbf39dc1-db53-45f3-c0fe-68e76d6842cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7345958637997538\n"
     ]
    }
   ],
   "source": [
    "# We'll build 10 trees\n",
    "tree_count = 10\n",
    "\n",
    "# Each \"bag\" will have 60% of the number of original rows\n",
    "bag_proportion = .6\n",
    "\n",
    "predictions = []\n",
    "for i in range(tree_count):\n",
    "    # We select 60% of the rows from train, sampling with replacement\n",
    "    # We set a random state to ensure we'll be able to replicate our results\n",
    "    # We set it to i instead of a fixed value so we don't get the same sample every time\n",
    "    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)\n",
    "    \n",
    "    # Fit a decision tree model to the \"bag\"\n",
    "    \n",
    "    # PUT YOUR CODE HERE\n",
    "    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2,\n",
    "                                splitter='random', max_features='auto')\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    clf.fit(bag[columns], bag[\"high_income\"])\n",
    "    \n",
    "    # Using the model, make predictions on the test data\n",
    "    predictions.append(clf.predict_proba(test[columns])[:,1])\n",
    "\n",
    "combined = np.sum(predictions, axis=0) / 10\n",
    "rounded = np.round(combined)\n",
    "\n",
    "print(roc_auc_score(test[\"high_income\"], rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qGaZ6LrJIOB"
   },
   "source": [
    "## Practice Putting it All Together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_EZEMHd7FnU9"
   },
   "source": [
    "\n",
    "\n",
    "Using random subsets from the previous section improved the accuracy over using bagging alone:\n",
    "\n",
    "| settings                                             | test AUC |\n",
    "|------------------------------------------------------|----------|\n",
    "| min_samples_leaf: 2                                  | 0.688    |\n",
    "| max_depth: 2                                         | 0.676    |\n",
    "| combined predictions                                 | 0.715    |\n",
    "| min_samples_leaf: 2, with bagging                    | 0.732    |\n",
    "| min_samples_leaf: 2, with bagging and random subsets | 0.734    |\n",
    "\n",
    "So far, we've demonstrated the two building blocks of **Random Forests**, **bagging** and **random feature subsets**. Luckily, we don't have to write code from scratch each time. Scikit-learn has a [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) class and a [RandomForestRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) class that enable us to train and test random forest models quickly.\n",
    "\n",
    "When we instantiate a **RandomForestClassifier**, we pass in an **n_estimators** parameter that indicates how many trees to build. While adding more trees usually improves accuracy, it also increases the overall time the model takes to train.\n",
    "\n",
    "**RandomForestClassifier** has a similar interface to **DecisionTreeClassifier**, and we can use the **fit()** and **predict()** methods to train and make predictions.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Fit **clf** to the training data and make predictions on the test data.\n",
    "- Compute and print the AUC score between the test predictions and the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7GzrYhRLGtii"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7347461391939776"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=5, random_state=1, \n",
    "                             min_samples_leaf=2)\n",
    "\n",
    "clf.fit(train[columns], train['high_income'])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "\n",
    "roc_auc_score(test['high_income'], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edu02zH2JIOG"
   },
   "source": [
    "## Tweaking Parameters to Increase Accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rV9pRJAlG7g-"
   },
   "source": [
    "\n",
    "\n",
    "Similar to **Decision Trees**, we can tweak some of the parameters for **Random Forests**, including:\n",
    "\n",
    "- **min_samples_leaf**\n",
    "- **min_samples_split**\n",
    "- **max_depth**\n",
    "- **max_leaf_nodes**\n",
    "\n",
    "These parameters apply to the individual trees in the model, and change how they are constructed. There are also parameters specific to the **Random Forest** that alter its overall construction:\n",
    "\n",
    "- **n_estimators**\n",
    "- **bootstrap** - \"Bootstrap aggregation\" is another name for bagging; this parameter indicates whether to turn it on (Defaults to True)\n",
    "\n",
    "Refer to the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for a full list of parameters.\n",
    "\n",
    "Tweaking parameters can increase the accuracy of the forest. The easiest tweak is to increase the number of estimators we use. This approach yields diminishing returns -- going from 10 trees to 100 will make a bigger difference than going from 100 to 500, which will make a bigger difference than going from 500 to 1000. The accuracy increase function is logarithmic, so increasing the number of trees beyond a certain number (usually 200) won't help much at all.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Increase **n_estimators** to 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 770,
     "status": "ok",
     "timestamp": 1542616817764,
     "user": {
      "displayName": "Ivanovitch Silva",
      "photoUrl": "https://lh4.googleusercontent.com/-baHwkIBEacY/AAAAAAAAAAI/AAAAAAAAFo0/aWRaXNQgy7Q/s64/photo.jpg",
      "userId": "06428777505436195303"
     },
     "user_tz": 180
    },
    "id": "diT2-zWdICzl",
    "outputId": "15cc3dd1-67f0-4fed-c11b-7bbf84ea8e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7379403213124711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=150, random_state=1, \n",
    "                             min_samples_leaf=2)\n",
    "\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qujXpgMOJION"
   },
   "source": [
    "## Reducing Overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_SaMdn9IgFv"
   },
   "source": [
    "\n",
    "\n",
    "We were able to improve the AUC from 0.735 to 0.738, but the model using 150 trees took much longer to train. While the extra training time is trivial on the data set we're working with right now, understanding this trade-off will help you when you're working with much larger data sets where the extra time could amount to hours or days!\n",
    "\n",
    "One of the major advantages of **Random Forests** over single **Decision Trees** is that they tend to overfit less. Although each individual Decision Tree in a Random Forest varies widely, the average of their predictions is less sensitive to the input data than a single tree is. This is because while one tree can construct an incorrect and overfit model, the average of 100 or more trees will be more likely to hone in on the signal and ignore the noise. The signal will be the same across all of the trees, whereas each tree will hone in on the noise differently. This means that the average will discard the noise and keep the signal.\n",
    "\n",
    "In the following code cell, you'll see that we've fit a single decision tree to the training set, and made predictions for both the training and testing sets. The AUC for the training set predictions is .819, while the AUC for the testing set is .714. The fact that the test AUC is much lower than the train AUC indicates that the model is overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1542617666646,
     "user": {
      "displayName": "Ivanovitch Silva",
      "photoUrl": "https://lh4.googleusercontent.com/-baHwkIBEacY/AAAAAAAAAAI/AAAAAAAAFo0/aWRaXNQgy7Q/s64/photo.jpg",
      "userId": "06428777505436195303"
     },
     "user_tz": 180
    },
    "id": "nCVshSxTLYos",
    "outputId": "e6aba5d9-a619-4dcd-e981-6f3d139191ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8192570489534683\n",
      "relationship has a importance of 0.35486065267354705\n",
      "education_num has a importance of 0.2238305467772321\n",
      "age has a importance of 0.16626910165605044\n",
      "hours_per_week has a importance of 0.09802596230288446\n",
      "occupation has a importance of 0.08434714139306755\n",
      "workclass has a importance of 0.039372047642714826\n",
      "race has a importance of 0.009447315730049248\n",
      "native_country has a importance of 0.009270711003041526\n",
      "sex has a importance of 0.007713429852542139\n",
      "marital_status has a importance of 0.006863090968870699\n",
      "0.7139325899284541\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=5)\n",
    "\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(train[columns])\n",
    "print(roc_auc_score(train[\"high_income\"], predictions))\n",
    "\n",
    "for score,name in sorted(zip(clf.feature_importances_,columns),reverse=True):\n",
    "    print('{} has a importance of {}'.format(name,score))\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JpEUaVkoLXy9"
   },
   "source": [
    "\n",
    "\n",
    "Now let's train a similar random forest model and contrast it with what we just did.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Fit **clf** to the training set and use it to **make predictions** on the **training** set.\n",
    "- Then, use it to **make predictions** on the **testing** set.\n",
    "- Print both AUC scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7V9pZgWSLvc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7917047295143252\n",
      "0.7498874343962398\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=150, \n",
    "                             random_state=1, \n",
    "                             min_samples_leaf=5)\n",
    "\n",
    "clf.fit(train[columns], train['high_income'])\n",
    "\n",
    "predictions = clf.predict(train[columns])\n",
    "print(roc_auc_score(train[\"high_income\"], predictions))\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: prediction bike rentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CB0XEOB3_syh"
   },
   "source": [
    "## Introduction to the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qoovFC2hA2D9"
   },
   "source": [
    "Many American cities have communal bike sharing stations where you can rent bicycles by the hour or day. Washington, D.C. is one of these cities. The District collects detailed data on the number of bicycles people rent by the hour and day.\n",
    "\n",
    "**Hadi Fanaee-T** at the [University of Porto](http://www.up.pt/) compiled this data into a CSV file, which you'll be working with in this project. The file contains **17380** rows, with each row representing the number of bike rentals for a single hour of a single day. You can download the data from the University of California, [Irvine's website](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset). \n",
    "\n",
    "Here's what the first five rows look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "colab_type": "code",
    "id": "78fbHgJcByL4",
    "outputId": "2ae84e96-26d3-4ef7-8375-be3dc5822bcb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bike_rentals = pd.read_csv(\"bike_rental_hour.csv\")\n",
    "bike_rentals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8bnElKcDEtx"
   },
   "source": [
    "Here are the descriptions for the relevant columns:\n",
    "\n",
    "- **instant** - A unique sequential ID number for each row\n",
    "- **dteday** - The date of the rentals\n",
    "- **season** - The season in which the rentals occurred (1:springer, 2:summer, 3:fall, 4:winter)\n",
    "- **yr** - The year the rentals occurred (0: 2011, 1:2012)\n",
    "- **mnth** - The month the rentals occurred (1 to 12)\n",
    "- **hr** - The hour the rentals occurred (0 to 23)\n",
    "- **holiday** - Whether or not the day was a holiday\n",
    "- **weekday** - The day of the week (as a number, 0 to 7)\n",
    "- **workingday** - Whether or not the day was a working day\n",
    "- **weathersit** - The weather (as a categorical variable)\n",
    "- **temp** - The temperature, on a 0-1 scale\n",
    "- **atemp** - The adjusted temperature\n",
    "- **hum** - The humidity, on a 0-1 scale\n",
    "- **windspeed** - The wind speed, on a 0-1 scale\n",
    "- **casual** - The number of casual riders (people who hadn't previously signed up with the bike sharing program)\n",
    "- **registered** - The number of registered riders (people who had already signed up)\n",
    "- **cnt** - The total number of bike rentals (casual + registered)\n",
    "\n",
    "In this mission, you'll try to **predict the total number of bikes people rented in a given hour**. You'll predict the **cnt** column using all of the other columns, except for **casual** and **registered**. To accomplish this, you'll create a few different machine learning models and evaluate their performance.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Make a **histogram** of the **cnt** column of **bike_rentals**, and take a look at the distribution of total rentals.\n",
    "- Use the **corr** method on the **bike_rentals** dataframe to explore how each column is correlated with **cnt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kbkEvxZMEk-z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fcdd53d7550>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bike_rentals.cnt.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>instant</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.404046</td>\n",
       "      <td>0.866014</td>\n",
       "      <td>0.489164</td>\n",
       "      <td>-0.004775</td>\n",
       "      <td>0.014723</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>-0.003416</td>\n",
       "      <td>-0.014198</td>\n",
       "      <td>0.136178</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>0.009577</td>\n",
       "      <td>-0.074505</td>\n",
       "      <td>0.158295</td>\n",
       "      <td>0.282046</td>\n",
       "      <td>0.278379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <td>0.404046</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010742</td>\n",
       "      <td>0.830386</td>\n",
       "      <td>-0.006117</td>\n",
       "      <td>-0.009585</td>\n",
       "      <td>-0.002335</td>\n",
       "      <td>0.013743</td>\n",
       "      <td>-0.014524</td>\n",
       "      <td>0.312025</td>\n",
       "      <td>0.319380</td>\n",
       "      <td>0.150625</td>\n",
       "      <td>-0.149773</td>\n",
       "      <td>0.120206</td>\n",
       "      <td>0.174226</td>\n",
       "      <td>0.178056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yr</th>\n",
       "      <td>0.866014</td>\n",
       "      <td>-0.010742</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010473</td>\n",
       "      <td>-0.003867</td>\n",
       "      <td>0.006692</td>\n",
       "      <td>-0.004485</td>\n",
       "      <td>-0.002196</td>\n",
       "      <td>-0.019157</td>\n",
       "      <td>0.040913</td>\n",
       "      <td>0.039222</td>\n",
       "      <td>-0.083546</td>\n",
       "      <td>-0.008740</td>\n",
       "      <td>0.142779</td>\n",
       "      <td>0.253684</td>\n",
       "      <td>0.250495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth</th>\n",
       "      <td>0.489164</td>\n",
       "      <td>0.830386</td>\n",
       "      <td>-0.010473</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005772</td>\n",
       "      <td>0.018430</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>-0.003477</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.201691</td>\n",
       "      <td>0.208096</td>\n",
       "      <td>0.164411</td>\n",
       "      <td>-0.135386</td>\n",
       "      <td>0.068457</td>\n",
       "      <td>0.122273</td>\n",
       "      <td>0.120638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr</th>\n",
       "      <td>-0.004775</td>\n",
       "      <td>-0.006117</td>\n",
       "      <td>-0.003867</td>\n",
       "      <td>-0.005772</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>-0.003498</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>-0.020203</td>\n",
       "      <td>0.137603</td>\n",
       "      <td>0.133750</td>\n",
       "      <td>-0.276498</td>\n",
       "      <td>0.137252</td>\n",
       "      <td>0.301202</td>\n",
       "      <td>0.374141</td>\n",
       "      <td>0.394071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>holiday</th>\n",
       "      <td>0.014723</td>\n",
       "      <td>-0.009585</td>\n",
       "      <td>0.006692</td>\n",
       "      <td>0.018430</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.102088</td>\n",
       "      <td>-0.252471</td>\n",
       "      <td>-0.017036</td>\n",
       "      <td>-0.027340</td>\n",
       "      <td>-0.030973</td>\n",
       "      <td>-0.010588</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.031564</td>\n",
       "      <td>-0.047345</td>\n",
       "      <td>-0.030927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday</th>\n",
       "      <td>0.001357</td>\n",
       "      <td>-0.002335</td>\n",
       "      <td>-0.004485</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>-0.003498</td>\n",
       "      <td>-0.102088</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035955</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>-0.001795</td>\n",
       "      <td>-0.008821</td>\n",
       "      <td>-0.037158</td>\n",
       "      <td>0.011502</td>\n",
       "      <td>0.032721</td>\n",
       "      <td>0.021578</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workingday</th>\n",
       "      <td>-0.003416</td>\n",
       "      <td>0.013743</td>\n",
       "      <td>-0.002196</td>\n",
       "      <td>-0.003477</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>-0.252471</td>\n",
       "      <td>0.035955</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044672</td>\n",
       "      <td>0.055390</td>\n",
       "      <td>0.054667</td>\n",
       "      <td>0.015688</td>\n",
       "      <td>-0.011830</td>\n",
       "      <td>-0.300942</td>\n",
       "      <td>0.134326</td>\n",
       "      <td>0.030284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weathersit</th>\n",
       "      <td>-0.014198</td>\n",
       "      <td>-0.014524</td>\n",
       "      <td>-0.019157</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>-0.020203</td>\n",
       "      <td>-0.017036</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.044672</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.102640</td>\n",
       "      <td>-0.105563</td>\n",
       "      <td>0.418130</td>\n",
       "      <td>0.026226</td>\n",
       "      <td>-0.152628</td>\n",
       "      <td>-0.120966</td>\n",
       "      <td>-0.142426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>0.136178</td>\n",
       "      <td>0.312025</td>\n",
       "      <td>0.040913</td>\n",
       "      <td>0.201691</td>\n",
       "      <td>0.137603</td>\n",
       "      <td>-0.027340</td>\n",
       "      <td>-0.001795</td>\n",
       "      <td>0.055390</td>\n",
       "      <td>-0.102640</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987672</td>\n",
       "      <td>-0.069881</td>\n",
       "      <td>-0.023125</td>\n",
       "      <td>0.459616</td>\n",
       "      <td>0.335361</td>\n",
       "      <td>0.404772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atemp</th>\n",
       "      <td>0.137615</td>\n",
       "      <td>0.319380</td>\n",
       "      <td>0.039222</td>\n",
       "      <td>0.208096</td>\n",
       "      <td>0.133750</td>\n",
       "      <td>-0.030973</td>\n",
       "      <td>-0.008821</td>\n",
       "      <td>0.054667</td>\n",
       "      <td>-0.105563</td>\n",
       "      <td>0.987672</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.051918</td>\n",
       "      <td>-0.062336</td>\n",
       "      <td>0.454080</td>\n",
       "      <td>0.332559</td>\n",
       "      <td>0.400929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hum</th>\n",
       "      <td>0.009577</td>\n",
       "      <td>0.150625</td>\n",
       "      <td>-0.083546</td>\n",
       "      <td>0.164411</td>\n",
       "      <td>-0.276498</td>\n",
       "      <td>-0.010588</td>\n",
       "      <td>-0.037158</td>\n",
       "      <td>0.015688</td>\n",
       "      <td>0.418130</td>\n",
       "      <td>-0.069881</td>\n",
       "      <td>-0.051918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.290105</td>\n",
       "      <td>-0.347028</td>\n",
       "      <td>-0.273933</td>\n",
       "      <td>-0.322911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>windspeed</th>\n",
       "      <td>-0.074505</td>\n",
       "      <td>-0.149773</td>\n",
       "      <td>-0.008740</td>\n",
       "      <td>-0.135386</td>\n",
       "      <td>0.137252</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.011502</td>\n",
       "      <td>-0.011830</td>\n",
       "      <td>0.026226</td>\n",
       "      <td>-0.023125</td>\n",
       "      <td>-0.062336</td>\n",
       "      <td>-0.290105</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.090287</td>\n",
       "      <td>0.082321</td>\n",
       "      <td>0.093234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>casual</th>\n",
       "      <td>0.158295</td>\n",
       "      <td>0.120206</td>\n",
       "      <td>0.142779</td>\n",
       "      <td>0.068457</td>\n",
       "      <td>0.301202</td>\n",
       "      <td>0.031564</td>\n",
       "      <td>0.032721</td>\n",
       "      <td>-0.300942</td>\n",
       "      <td>-0.152628</td>\n",
       "      <td>0.459616</td>\n",
       "      <td>0.454080</td>\n",
       "      <td>-0.347028</td>\n",
       "      <td>0.090287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.506618</td>\n",
       "      <td>0.694564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>registered</th>\n",
       "      <td>0.282046</td>\n",
       "      <td>0.174226</td>\n",
       "      <td>0.253684</td>\n",
       "      <td>0.122273</td>\n",
       "      <td>0.374141</td>\n",
       "      <td>-0.047345</td>\n",
       "      <td>0.021578</td>\n",
       "      <td>0.134326</td>\n",
       "      <td>-0.120966</td>\n",
       "      <td>0.335361</td>\n",
       "      <td>0.332559</td>\n",
       "      <td>-0.273933</td>\n",
       "      <td>0.082321</td>\n",
       "      <td>0.506618</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnt</th>\n",
       "      <td>0.278379</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.250495</td>\n",
       "      <td>0.120638</td>\n",
       "      <td>0.394071</td>\n",
       "      <td>-0.030927</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.030284</td>\n",
       "      <td>-0.142426</td>\n",
       "      <td>0.404772</td>\n",
       "      <td>0.400929</td>\n",
       "      <td>-0.322911</td>\n",
       "      <td>0.093234</td>\n",
       "      <td>0.694564</td>\n",
       "      <td>0.972151</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             instant    season        yr      mnth        hr   holiday  \\\n",
       "instant     1.000000  0.404046  0.866014  0.489164 -0.004775  0.014723   \n",
       "season      0.404046  1.000000 -0.010742  0.830386 -0.006117 -0.009585   \n",
       "yr          0.866014 -0.010742  1.000000 -0.010473 -0.003867  0.006692   \n",
       "mnth        0.489164  0.830386 -0.010473  1.000000 -0.005772  0.018430   \n",
       "hr         -0.004775 -0.006117 -0.003867 -0.005772  1.000000  0.000479   \n",
       "holiday     0.014723 -0.009585  0.006692  0.018430  0.000479  1.000000   \n",
       "weekday     0.001357 -0.002335 -0.004485  0.010400 -0.003498 -0.102088   \n",
       "workingday -0.003416  0.013743 -0.002196 -0.003477  0.002285 -0.252471   \n",
       "weathersit -0.014198 -0.014524 -0.019157  0.005400 -0.020203 -0.017036   \n",
       "temp        0.136178  0.312025  0.040913  0.201691  0.137603 -0.027340   \n",
       "atemp       0.137615  0.319380  0.039222  0.208096  0.133750 -0.030973   \n",
       "hum         0.009577  0.150625 -0.083546  0.164411 -0.276498 -0.010588   \n",
       "windspeed  -0.074505 -0.149773 -0.008740 -0.135386  0.137252  0.003988   \n",
       "casual      0.158295  0.120206  0.142779  0.068457  0.301202  0.031564   \n",
       "registered  0.282046  0.174226  0.253684  0.122273  0.374141 -0.047345   \n",
       "cnt         0.278379  0.178056  0.250495  0.120638  0.394071 -0.030927   \n",
       "\n",
       "             weekday  workingday  weathersit      temp     atemp       hum  \\\n",
       "instant     0.001357   -0.003416   -0.014198  0.136178  0.137615  0.009577   \n",
       "season     -0.002335    0.013743   -0.014524  0.312025  0.319380  0.150625   \n",
       "yr         -0.004485   -0.002196   -0.019157  0.040913  0.039222 -0.083546   \n",
       "mnth        0.010400   -0.003477    0.005400  0.201691  0.208096  0.164411   \n",
       "hr         -0.003498    0.002285   -0.020203  0.137603  0.133750 -0.276498   \n",
       "holiday    -0.102088   -0.252471   -0.017036 -0.027340 -0.030973 -0.010588   \n",
       "weekday     1.000000    0.035955    0.003311 -0.001795 -0.008821 -0.037158   \n",
       "workingday  0.035955    1.000000    0.044672  0.055390  0.054667  0.015688   \n",
       "weathersit  0.003311    0.044672    1.000000 -0.102640 -0.105563  0.418130   \n",
       "temp       -0.001795    0.055390   -0.102640  1.000000  0.987672 -0.069881   \n",
       "atemp      -0.008821    0.054667   -0.105563  0.987672  1.000000 -0.051918   \n",
       "hum        -0.037158    0.015688    0.418130 -0.069881 -0.051918  1.000000   \n",
       "windspeed   0.011502   -0.011830    0.026226 -0.023125 -0.062336 -0.290105   \n",
       "casual      0.032721   -0.300942   -0.152628  0.459616  0.454080 -0.347028   \n",
       "registered  0.021578    0.134326   -0.120966  0.335361  0.332559 -0.273933   \n",
       "cnt         0.026900    0.030284   -0.142426  0.404772  0.400929 -0.322911   \n",
       "\n",
       "            windspeed    casual  registered       cnt  \n",
       "instant     -0.074505  0.158295    0.282046  0.278379  \n",
       "season      -0.149773  0.120206    0.174226  0.178056  \n",
       "yr          -0.008740  0.142779    0.253684  0.250495  \n",
       "mnth        -0.135386  0.068457    0.122273  0.120638  \n",
       "hr           0.137252  0.301202    0.374141  0.394071  \n",
       "holiday      0.003988  0.031564   -0.047345 -0.030927  \n",
       "weekday      0.011502  0.032721    0.021578  0.026900  \n",
       "workingday  -0.011830 -0.300942    0.134326  0.030284  \n",
       "weathersit   0.026226 -0.152628   -0.120966 -0.142426  \n",
       "temp        -0.023125  0.459616    0.335361  0.404772  \n",
       "atemp       -0.062336  0.454080    0.332559  0.400929  \n",
       "hum         -0.290105 -0.347028   -0.273933 -0.322911  \n",
       "windspeed    1.000000  0.090287    0.082321  0.093234  \n",
       "casual       0.090287  1.000000    0.506618  0.694564  \n",
       "registered   0.082321  0.506618    1.000000  0.972151  \n",
       "cnt          0.093234  0.694564    0.972151  1.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_rentals.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Tp1N9fzFRBt"
   },
   "source": [
    "## Calculating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1awGqWnFSTl"
   },
   "source": [
    "It can often be helpful to calculate **features** before applying machine learning models. **Features** can enhance the accuracy of models by introducing new information, or distilling existing information.\n",
    "\n",
    "For example, the **hr** column in **bike_rentals** contains the hours during which bikes are rented, from 1 to 24. A machine will treat each hour differently, without understanding that certain hours are related. We can introduce some order into the process by creating a new column with labels for **morning**, **afternoon**, **evening**, and **night**. This will bundle similar times together, enabling the model to make better decisions.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Write a function called **assign_label** that takes in a numeric value for an hour, and returns:\n",
    "  - 1 if the hour is from 6 to 12\n",
    "  - 2 if the hour is from 12 to 18\n",
    "  - 3 if the hour is from 18 to 24\n",
    "  - 4 if the hour is from 0 to 6\n",
    "- Use the [apply](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html) method on series objects to apply the function to each item in the **hr** column.\n",
    "- Assign the result to the **time_label** column of **bike_rentals.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0M5Mq3XFsHk"
   },
   "outputs": [],
   "source": [
    "def assign_label(value):\n",
    "    if value < 6:\n",
    "        return 4\n",
    "    elif value < 12:\n",
    "        return 1\n",
    "    elif value < 18:\n",
    "        return 2\n",
    "    elif value < 24:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rentals['time_label'] = bike_rentals.hr.apply(assign_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_lOEda-Gb_I"
   },
   "source": [
    "## Splitting the Data Into Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vwv_cORLGfyf"
   },
   "source": [
    "Before you can begin applying machine learning algorithms, you'll need to split the data into **training** and **testing** sets. This will enable you to train an algorithm using the training set, and evaluate its accuracy on the testing set. If you train an algorithm on the training data, then evaluate its performance on the same data, you can get an unrealistically low error value, due to overfitting.\n",
    "\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Based on your explorations of the **cnt** column, pick an **error metric** you want to use to evaluate the performance of the machine learning algorithms. Explain why you chose this metric in a markdown cell.\n",
    "- Select 80% of the rows in **bike_rentals** to be part of the **training set** using the [sample method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html) on **bike_rentals**. Assign the result to **train.**\n",
    "- Select the rows that are in **bike_rentals** but not in train to be in the **testing set**. Assign the result to **test.**\n",
    "    - This line will generate a Boolean series that's **False** when a row in **bike_rentals** isn't found in **train**:\n",
    "    ```python\n",
    "    bike_rentals.index.isin(train.index)\n",
    "    ```\n",
    "    - This line will select any rows in **bike_rentals** that aren't found in **train** to be in the testing set: \n",
    "    ```python\n",
    "    bike_rentals.loc[~bike_rentals.index.isin(train.index)]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTnG6J6aGlHu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cols = ['instant', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n",
    "       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n",
    "       'time_label']\n",
    "\n",
    "# train, test = train_test_split(bike_rentals, test_size=0.2)\n",
    "train = bike_rentals.sample(n=int(bike_rentals.shape[0]*.8))\n",
    "test = bike_rentals[~bike_rentals.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3476, 18)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've done some exploration and manipulation, you're ready to apply **linear regression** to the data. Linear regression will probably work fairly well on this data, given that many of the columns are highly correlated with **cnt**.\n",
    "\n",
    "As you learned in earlier lessons, **linear regression works best when predictors are linearly correlated to the target** and also independent -- in other words, they don't change meaning when we combine them with each other. The good thing about linear regression is that it's fairly resistant to overfitting because it's straightforward. It also can be prone to underfitting the data, however, and not building a powerful enough model. This means that **linear regression usually isn't the most accurate option.**\n",
    "\n",
    "You'll need to **ignore** the **casual** and **registered** columns because **cnt** is derived from them. If you're trying to predict the number of people who rent bikes in a given hour (cnt), it doesn't make sense that you'd already know casual or registered, because those numbers are added together to get cnt.\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Create a list of predictor columns to use in training and predictions.\n",
    "    - At a minimum, this list should exclude the **cnt**, **casual**, **dteday**, and **registered** columns.\n",
    "    - Feel free to remove other columns you don't think will be useful for the predictions.\n",
    "- Use the [LinearRegression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from sklearn to train a machine learning algorithm on **train**.\n",
    "    - Use only the columns in the list of predictors for this.\n",
    "- Make predictions using the [LinearRegression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) on **test.**\n",
    "    - Use the exact same predictor columns here.\n",
    "- Calculate the error between the predictions and the actual values.\n",
    "- Write your thoughts about the predictions and the error in a markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "reg.fit(train[cols], train['cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127.66832239012227"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions = reg.predict(test[cols])\n",
    "\n",
    "np.sqrt(np.mean((predictions - test['cnt']) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to apply the **decision tree** algorithm. You'll be able to compare its error with the error from linear regression, which will enable you to pick the right algorithm for this data set.\n",
    "\n",
    "**Decision trees** tend to predict outcomes much more reliably than linear regression models. Because a decision tree is a fairly complex model, it also tends to overfit, particularly when we don't tweak parameters like **maximum depth** and **minimum number of samples** per leaf. Decision trees are also prone to instability -- small changes in the input data can result in a very different output model.\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Use the [DecisionTreeRegressor class](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) to fit a decision tree algorithm to the **train** data.\n",
    "- Make predictions using the [DecisionTreeRegressor class](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) on **test**.\n",
    "- Calculate the error between the predictions and the actual values.\n",
    "- Experiment with various parameters of the [DecisionTreeRegressor class](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html), including **min_samples_leaf**, to see if it changes the error.\n",
    "- Write your thoughts on the predictions and the error in a markdown cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.276190448810596"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "reg = DecisionTreeRegressor(min_samples_leaf=2)\n",
    "\n",
    "reg.fit(train[cols], train['cnt'])\n",
    "\n",
    "predictions = reg.predict(test[cols])\n",
    "\n",
    "np.sqrt(np.mean((predictions - test['cnt']) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now apply the **random forest** algorithm, which improves on the decision tree algorithm. **Random forests tend to be much more accurate than simple models like linear regression**. Due to the way random forests are constructed, they tend to overfit much less than decision trees. Random forests can still be prone to overfitting, though, so it's important to tune parameters like **maximum depth** and **minimum samples per leaf**.\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Use the [RandomForestRegressor class](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) to fit a random forest algorithm to the **train** data.\n",
    "- Make predictions using the **RandomForestRegressor** class on **test**.\n",
    "- Calculate the error between the predictions and the actual values.\n",
    "- Experiment with various parameters of the **RandomForestRegressor** class, including **min_samples_leaf**, to see if it changes the error.\n",
    "- Write your thoughts on the predictions and the error in a markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.08820595736593"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "reg = RandomForestRegressor(n_estimators=100, min_samples_leaf=2)\n",
    "\n",
    "reg.fit(train[cols], train['cnt'])\n",
    "\n",
    "predict = reg.predict(test[cols])\n",
    "\n",
    "np.sqrt(np.mean((predict - test['cnt']) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend exploring the data more on your own.\n",
    "\n",
    "Here are some potential next steps:\n",
    "\n",
    "- Calculate additional features, such as:\n",
    "    - An index combining **temperature**, **humidity**, and **wind speed**\n",
    "- Try predicting **casual** and **registered** instead of **cnt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GD7oOSWqJIOe"
   },
   "source": [
    "# When to Use Random Forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcfesuIyMOY7"
   },
   "source": [
    "\n",
    "\n",
    "As we can see in the code cell from the previous section, overfitting decreased with a Random Forest, and accuracy went up overall.\n",
    "\n",
    "While the random forest algorithm is incredibly powerful, it isn't applicable to all tasks. The main **strengths** of a **Random Forest** are:\n",
    "\n",
    "- **Very accurate predictions** - Random forests achieve near state-of-the-art performance on many machine learning tasks. Along with neural networks and gradient-boosted trees, they're typically one of the top-performing algorithms.\n",
    "- **Resistance to overfitting** - Due to their construction, Random Forests are fairly resistant to overfitting. We still need to set and tweak parameters like **max_depth** though.\n",
    "\n",
    "The main **weaknesses** of using a **Random Forest** are:\n",
    "\n",
    "- **They're difficult to interpret** - Because we've averaging the results of many trees, it can be hard to figure out why a Random Forest is making predictions the way it is.\n",
    "- **They take longer to create** - Making two trees takes twice as long as making one, making three takes three times as long, and so on. Fortunately, we can exploit multicore processors to parallelize tree construction. Scikit allows us to do this through the **n_jobs** parameter on [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). We'll discuss parallelization in greater detail later on.\n",
    "\n",
    "Given these trade-offs, it makes sense to use Random Forests in situations where accuracy is of the utmost importance; being able to interpret or explain the decisions the model is making isn't key. In cases where time is of the essence or interpretability is important, a single decision tree may be a better choice."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lesson #10 -  Introduction to Random Forests.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
